{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjeevtrivedi/pgd-dsai/blob/main/NLP_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to today’s session on text processing using NLTK and Gensim. We’ll cover core preprocessing steps, feature extraction, and building Word2Vec embeddings."
      ],
      "metadata": {
        "id": "PO6DnvSWJElD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMeE7i4jJAjp",
        "outputId": "1c890b49-c291-4683-c343-85d605d1b713"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Install libraries (run once per session)\n",
        "!pip install --quiet nltk gensim\n",
        "!pip install --quiet numpy scipy scikit-learn\n",
        "# Download NLTK data\n",
        "import nltk\n",
        "nltk.download('punkt_tab')          # Pre-trained tokenizers for splitting text into words and sentences.\n",
        "nltk.download('stopwords')     #  collection of common stopwords in many languages (like “the”, “and”, “is”)\n",
        "nltk.download('wordnet')       # Used for lemmatization — converting words to their base form (e.g., \"running\" → \"run\").\n",
        "nltk.download('averaged_perceptron_tagger')  # Tags words as nouns, verbs, adjectives, etc., which helps in better lemmatization and grammar analysis.\n",
        "nltk.download('tagsets')    # Detailed descriptions of POS tag symbols (e.g., “NN” = Noun, “VB” = Verb).\n",
        "# Download the specific resource if needed\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. NLTK Basics: Tokenization & Text Cleanup\n",
        "Tokenization splits text into meaningful units. We’ll then remove stopwords, apply stemming/lemmatization, and tag parts‑of‑speech."
      ],
      "metadata": {
        "id": "WZqBcp_TJLz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "text = \"\"\"\n",
        "Natural Language Processing (NLP) enables computers to understand human language.\n",
        "In this tutorial, we'll see how to preprocess text with NLTK.\n",
        "\"\"\"\n",
        "\n",
        "# Sentence & word tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "words = word_tokenize(text)\n",
        "print(\"Sentences:\\n\", sentences)\n",
        "print(\"Words:\\n\", words)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered = [w for w in words if w.lower() not in stop_words and w.isalnum()]\n",
        "print(\"Filtered:\\n\", filtered)\n",
        "\n",
        "# Stemming vs Lemmatization\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stemmed = [ps.stem(w) for w in filtered]\n",
        "lemmatized = [lemmatizer.lemmatize(w) for w in filtered]\n",
        "print(\"Stemmed:\\n\", stemmed)\n",
        "print(\"Lemmatized:\\n\", lemmatized)\n",
        "\n",
        "# POS tagging\n",
        "pos_tags = pos_tag(filtered)      # Tags each word with its grammatical role (noun, verb, adjective, etc.)\n",
        "print(\"POS Tags:\\n\", pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRPtY_GlJTvl",
        "outputId": "e2c5fc57-0291-47bb-8cc8-8e8ada799bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            " ['\\nNatural Language Processing (NLP) enables computers to understand human language.', \"In this tutorial, we'll see how to preprocess text with NLTK.\"]\n",
            "Words:\n",
            " ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'enables', 'computers', 'to', 'understand', 'human', 'language', '.', 'In', 'this', 'tutorial', ',', 'we', \"'ll\", 'see', 'how', 'to', 'preprocess', 'text', 'with', 'NLTK', '.']\n",
            "Filtered:\n",
            " ['Natural', 'Language', 'Processing', 'NLP', 'enables', 'computers', 'understand', 'human', 'language', 'tutorial', 'see', 'preprocess', 'text', 'NLTK']\n",
            "Stemmed:\n",
            " ['natur', 'languag', 'process', 'nlp', 'enabl', 'comput', 'understand', 'human', 'languag', 'tutori', 'see', 'preprocess', 'text', 'nltk']\n",
            "Lemmatized:\n",
            " ['Natural', 'Language', 'Processing', 'NLP', 'enables', 'computer', 'understand', 'human', 'language', 'tutorial', 'see', 'preprocess', 'text', 'NLTK']\n",
            "POS Tags:\n",
            " [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('NLP', 'NNP'), ('enables', 'VBZ'), ('computers', 'NNS'), ('understand', 'VBP'), ('human', 'JJ'), ('language', 'NN'), ('tutorial', 'JJ'), ('see', 'NN'), ('preprocess', 'JJ'), ('text', 'NN'), ('NLTK', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.data import load\n",
        "\n",
        "# Load the tagset\n",
        "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
        "\n",
        "# Print the list\n",
        "for tag in sorted(tagdict):\n",
        "    print(f\"{tag}: {tagdict[tag][0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV6azF3n5jJ2",
        "outputId": "56ab576f-bc5e-45fc-d1e4-11fc51b951f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$: dollar\n",
            "'': closing quotation mark\n",
            "(: opening parenthesis\n",
            "): closing parenthesis\n",
            ",: comma\n",
            "--: dash\n",
            ".: sentence terminator\n",
            ":: colon or ellipsis\n",
            "CC: conjunction, coordinating\n",
            "CD: numeral, cardinal\n",
            "DT: determiner\n",
            "EX: existential there\n",
            "FW: foreign word\n",
            "IN: preposition or conjunction, subordinating\n",
            "JJ: adjective or numeral, ordinal\n",
            "JJR: adjective, comparative\n",
            "JJS: adjective, superlative\n",
            "LS: list item marker\n",
            "MD: modal auxiliary\n",
            "NN: noun, common, singular or mass\n",
            "NNP: noun, proper, singular\n",
            "NNPS: noun, proper, plural\n",
            "NNS: noun, common, plural\n",
            "PDT: pre-determiner\n",
            "POS: genitive marker\n",
            "PRP: pronoun, personal\n",
            "PRP$: pronoun, possessive\n",
            "RB: adverb\n",
            "RBR: adverb, comparative\n",
            "RBS: adverb, superlative\n",
            "RP: particle\n",
            "SYM: symbol\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "UH: interjection\n",
            "VB: verb, base form\n",
            "VBD: verb, past tense\n",
            "VBG: verb, present participle or gerund\n",
            "VBN: verb, past participle\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "WDT: WH-determiner\n",
            "WP: WH-pronoun\n",
            "WP$: WH-pronoun, possessive\n",
            "WRB: Wh-adverb\n",
            "``: opening quotation mark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for tup in pos_tags:\n",
        "  print(tup[0],\"----->\",tagdict[tup[1]][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO-QoKZP6DHE",
        "outputId": "3b98a30e-3075-4d12-807e-149582bf1052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural -----> adjective or numeral, ordinal\n",
            "Language -----> noun, proper, singular\n",
            "Processing -----> noun, proper, singular\n",
            "NLP -----> noun, proper, singular\n",
            "enables -----> verb, present tense, 3rd person singular\n",
            "computers -----> noun, common, plural\n",
            "understand -----> verb, present tense, not 3rd person singular\n",
            "human -----> adjective or numeral, ordinal\n",
            "language -----> noun, common, singular or mass\n",
            "tutorial -----> adjective or numeral, ordinal\n",
            "see -----> noun, common, singular or mass\n",
            "preprocess -----> adjective or numeral, ordinal\n",
            "text -----> noun, common, singular or mass\n",
            "NLTK -----> noun, proper, singular\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example=\"The quick brown fox jumps over the lazy dog.\"\n",
        "# Sentence & word tokenization\n",
        "sentences = sent_tokenize(example)\n",
        "words = word_tokenize(example)\n",
        "print(\"Sentences:\\n\", sentences)\n",
        "print(\"Words:\\n\", words)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered = [w for w in words if w.lower() not in stop_words and w.isalnum()]\n",
        "print(\"Filtered:\\n\", filtered)\n",
        "\n",
        "# Stemming vs Lemmatization\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stemmed = [ps.stem(w) for w in filtered]\n",
        "lemmatized = [lemmatizer.lemmatize(w) for w in filtered]\n",
        "print(\"Stemmed:\\n\", stemmed)\n",
        "print(\"Lemmatized:\\n\", lemmatized)\n",
        "\n",
        "# POS tagging\n",
        "pos_tags = pos_tag(filtered)\n",
        "print(\"POS Tags:\\n\", pos_tags)\n",
        "print(\"\\nPOS Tags in a bit detail:\\n\")\n",
        "for tup in pos_tags:\n",
        "  print(tup[0],\"----->\",tagdict[tup[1]][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4hKiSEy8E4f",
        "outputId": "3c1d16ff-b757-4ed5-bb03-282b6d92e8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            " ['The quick brown fox jumps over the lazy dog.']\n",
            "Words:\n",
            " ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
            "Filtered:\n",
            " ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n",
            "Stemmed:\n",
            " ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n",
            "Lemmatized:\n",
            " ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog']\n",
            "POS Tags:\n",
            " [('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NNS'), ('lazy', 'JJ'), ('dog', 'NN')]\n",
            "\n",
            "POS Tags in a bit detail:\n",
            "\n",
            "quick -----> adjective or numeral, ordinal\n",
            "brown -----> noun, common, singular or mass\n",
            "fox -----> noun, common, singular or mass\n",
            "jumps -----> noun, common, plural\n",
            "lazy -----> adjective or numeral, ordinal\n",
            "dog -----> noun, common, singular or mass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Difference between Lemmatization & Stemming"
      ],
      "metadata": {
        "id": "xuKD2ain4Hvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Sample text\n",
        "text = \"The striped bats are hanging on their feet for best\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Initialize the stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Function to get the part of speech tag for lemmatization\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Apply lemmatization\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
        "\n",
        "# Print results\n",
        "print(\"Original Text: \", text)\n",
        "print(\"Tokenized Words: \", words)\n",
        "print(\"Stemmed Words: \", stemmed_words)\n",
        "print(\"Lemmatized Words: \", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q37Ocq5A4Lzp",
        "outputId": "0e4501d1-9e33-436f-d30a-9c0ac5be7950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:  The striped bats are hanging on their feet for best\n",
            "Tokenized Words:  ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
            "Stemmed Words:  ['the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'feet', 'for', 'best']\n",
            "Lemmatized Words:  ['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# When to Use Lemmatization vs. Stemming\n",
        "The choice between lemmatization and stemming depends on the specific requirements of the NLP task at hand:\n",
        "\n",
        "## Use Lemmatization When:\n",
        "* Accuracy and context are crucial.\n",
        "* The task involves complex language understanding, such as chatbots, sentiment analysis, and machine translation.\n",
        "* The computational resources are sufficient to handle the additional complexity.\n",
        "## Use Stemming When:\n",
        "* Speed and efficiency are more important than accuracy.\n",
        "* The task involves simple text normalization, such as search engines and information retrieval systems.\n",
        "* The computational resources are limited"
      ],
      "metadata": {
        "id": "3cLkb0nz5CX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Text Vectorization (NLTK & sklearn)\n",
        "Converting text into numeric features: Bag‑of‑Words, TF‑IDF. NLTK offers TextCollection, while sklearn provides CountVectorizer/TfidfVectorizer."
      ],
      "metadata": {
        "id": "g841-Eg5JUiE"
      }
    },
    {
      "source": [
        "!pip install --upgrade numpy==1.23.5 scikit-learn==1.1.3"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZTvYsl9-_Hu",
        "outputId": "ef97224d-5444-4c35-db77-ae404be03be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Requirement already satisfied: scikit-learn==1.1.3 in /usr/local/lib/python3.11/dist-packages (1.1.3)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.1.3) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.1.3) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.1.3) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.text import TextCollection         # TextCollection turns a list of documents into an NLTK-readable corpus and allows tf-idf queries\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Sample documents\n",
        "docs = [\n",
        "    \"Climate change is affecting global temperatures and weather patterns.\",\n",
        "    \"Renewable energy sources like solar and wind reduce carbon emissions.\",\n",
        "    \"Fossil fuels contribute heavily to greenhouse gas emissions.\",\n",
        "    \"Investing in solar energy helps combat climate change effectively.\",\n",
        "]\n",
        "\n",
        "# --- NLTK TextCollection for TF-IDF ---\n",
        "corpus = TextCollection(docs)\n",
        "# Check TF-IDF of selected terms in document 4\n",
        "target_doc = docs[3]\n",
        "for word in ['climate', 'solar', 'emissions']:\n",
        "    tfidf = corpus.tf_idf(word, target_doc)\n",
        "    print(f\"NLTK TF‑IDF('{word}') in doc4: {tfidf:.4f}\")\n",
        "\n",
        "# --- sklearn CountVectorizer & TfidfVectorizer ---\n",
        "cv = CountVectorizer(stop_words='english')\n",
        "X_counts = cv.fit_transform(docs)      # Builds a vocabulary of words and converts each document into a Bag-of-Words count vector.\n",
        "print(\"Vocabulary:\\n\", cv.vocabulary_)\n",
        "print(\"Count Matrix:\\n\", X_counts.toarray())\n",
        "\n",
        "tfv = TfidfVectorizer(stop_words='english')\n",
        "X_tfidf = tfv.fit_transform(docs)\n",
        "print(\"TF‑IDF Matrix:\\n\", X_tfidf.toarray())\n",
        "# print((X_tfidf.toarray()[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kxy35_IYJZjr",
        "outputId": "b8bfe4a5-babe-445b-b7b7-4cf32425886c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK TF‑IDF('climate') in doc4: 0.0210\n",
            "NLTK TF‑IDF('solar') in doc4: 0.0105\n",
            "NLTK TF‑IDF('emissions') in doc4: 0.0000\n",
            "Vocabulary:\n",
            " {'climate': 3, 'change': 2, 'affecting': 0, 'global': 12, 'temperatures': 23, 'weather': 24, 'patterns': 18, 'renewable': 20, 'energy': 8, 'sources': 22, 'like': 17, 'solar': 21, 'wind': 25, 'reduce': 19, 'carbon': 1, 'emissions': 7, 'fossil': 9, 'fuels': 10, 'contribute': 5, 'heavily': 14, 'greenhouse': 13, 'gas': 11, 'investing': 16, 'helps': 15, 'combat': 4, 'effectively': 6}\n",
            "Count Matrix:\n",
            " [[1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0]\n",
            " [0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1]\n",
            " [0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0]]\n",
            "TF‑IDF Matrix:\n",
            " [[0.40021825 0.         0.31553666 0.31553666 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.40021825 0.         0.         0.         0.         0.\n",
            "  0.40021825 0.         0.         0.         0.         0.40021825\n",
            "  0.40021825 0.        ]\n",
            " [0.         0.35657982 0.         0.         0.         0.\n",
            "  0.         0.28113163 0.28113163 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.35657982\n",
            "  0.         0.35657982 0.35657982 0.28113163 0.35657982 0.\n",
            "  0.         0.35657982]\n",
            " [0.         0.         0.         0.         0.         0.38861429\n",
            "  0.         0.30638797 0.         0.38861429 0.38861429 0.38861429\n",
            "  0.         0.38861429 0.38861429 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.30956515 0.30956515 0.39264414 0.\n",
            "  0.39264414 0.         0.30956515 0.         0.         0.\n",
            "  0.         0.         0.         0.39264414 0.39264414 0.\n",
            "  0.         0.         0.         0.30956515 0.         0.\n",
            "  0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Gensim Overview: Corpus, TF‑IDF, Word2Vec\n",
        "\n",
        "Gensim excels at unsupervised topic modeling and vector embedding techniques. We’ll create a Dictionary & Corpus, apply TF‑IDF, and train a Word2Vec model."
      ],
      "metadata": {
        "id": "R7ZvkKVEJiJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora, models\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "docs = [\n",
        "    \"Climate change is affecting global temperatures and weather patterns.\",\n",
        "    \"Renewable energy sources like solar and wind reduce carbon emissions.\",\n",
        "    \"Fossil fuels contribute heavily to greenhouse gas emissions.\",\n",
        "    \"Investing in solar energy helps combat climate change effectively.\",\n",
        "]\n",
        "\n",
        "# Preprocess docs for Gensim\n",
        "processed = [[w.lower() for w in word_tokenize(doc) if w.isalpha()] for doc in docs]\n",
        "print(processed)\n",
        "\n",
        "# Create Dictionary & Corpus\n",
        "dictionary = corpora.Dictionary(processed)\n",
        "corpus = [dictionary.doc2bow(text) for text in processed]\n",
        "print(\"Dictionary token2id:\\n\", dictionary.token2id)\n",
        "print(\"Corpus (BoW):\\n\", corpus)      # This is Count Frequency for each word\n",
        "\n",
        "# TF-IDF Model\n",
        "tfidf = models.TfidfModel(corpus)\n",
        "corpus_tfidf = tfidf[corpus]\n",
        "print(\"TF‑IDF Corpus:\\n\", list(corpus_tfidf))\n",
        "\n",
        "# Word2Vec Model\n",
        "w2v_model = Word2Vec(sentences=processed, vector_size=50, window=5, min_count=1, workers=2)\n",
        "\n",
        "# Check if the word is in the vocabulary before accessing it\n",
        "target_word = 'language'\n",
        "if target_word in w2v_model.wv:\n",
        "    print(f\"Vector for '{target_word}':\\n\", w2v_model.wv[target_word])\n",
        "    print(f\"Most similar to '{target_word}':\\n\", w2v_model.wv.most_similar(target_word))\n",
        "else:\n",
        "    print(f\"'{target_word}' not found in the Word2Vec vocabulary.\")\n",
        "\n",
        "# Save & load model\n",
        "w2v_model.save('w2v.model')\n",
        "loaded = Word2Vec.load('w2v.model')\n",
        "print(\"Loaded model vocab size:\", len(loaded.wv))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t1y-6rBJnMU",
        "outputId": "077d9f35-3be8-459b-8bca-f96258ea1688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['climate', 'change', 'is', 'affecting', 'global', 'temperatures', 'and', 'weather', 'patterns'], ['renewable', 'energy', 'sources', 'like', 'solar', 'and', 'wind', 'reduce', 'carbon', 'emissions'], ['fossil', 'fuels', 'contribute', 'heavily', 'to', 'greenhouse', 'gas', 'emissions'], ['investing', 'in', 'solar', 'energy', 'helps', 'combat', 'climate', 'change', 'effectively']]\n",
            "Dictionary token2id:\n",
            " {'affecting': 0, 'and': 1, 'change': 2, 'climate': 3, 'global': 4, 'is': 5, 'patterns': 6, 'temperatures': 7, 'weather': 8, 'carbon': 9, 'emissions': 10, 'energy': 11, 'like': 12, 'reduce': 13, 'renewable': 14, 'solar': 15, 'sources': 16, 'wind': 17, 'contribute': 18, 'fossil': 19, 'fuels': 20, 'gas': 21, 'greenhouse': 22, 'heavily': 23, 'to': 24, 'combat': 25, 'effectively': 26, 'helps': 27, 'in': 28, 'investing': 29}\n",
            "Corpus (BoW):\n",
            " [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)], [(1, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)], [(10, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1)], [(2, 1), (3, 1), (11, 1), (15, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)]]\n",
            "TF‑IDF Corpus:\n",
            " [[(0, 0.3849001794597505), (1, 0.19245008972987526), (2, 0.19245008972987526), (3, 0.19245008972987526), (4, 0.3849001794597505), (5, 0.3849001794597505), (6, 0.3849001794597505), (7, 0.3849001794597505), (8, 0.3849001794597505)], [(1, 0.1889822365046136), (9, 0.3779644730092272), (10, 0.1889822365046136), (11, 0.1889822365046136), (12, 0.3779644730092272), (13, 0.3779644730092272), (14, 0.3779644730092272), (15, 0.1889822365046136), (16, 0.3779644730092272), (17, 0.3779644730092272)], [(10, 0.18569533817705186), (18, 0.3713906763541037), (19, 0.3713906763541037), (20, 0.3713906763541037), (21, 0.3713906763541037), (22, 0.3713906763541037), (23, 0.3713906763541037), (24, 0.3713906763541037)], [(2, 0.20412414523193154), (3, 0.20412414523193154), (11, 0.20412414523193154), (15, 0.20412414523193154), (25, 0.4082482904638631), (26, 0.4082482904638631), (27, 0.4082482904638631), (28, 0.4082482904638631), (29, 0.4082482904638631)]]\n",
            "'language' not found in the Word2Vec vocabulary.\n",
            "Loaded model vocab size: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Hands‑On Lab\n",
        "\n",
        "Let’s build a small end-to-end pipeline:\n",
        "\n",
        "* Load a sample corpus (e.g., 5 news headlines).\n",
        "\n",
        "* Preprocess (tokenize, remove stopwords, lemmatize).\n",
        "\n",
        "* Vectorize with TF‑IDF.\n",
        "\n",
        "* Train a Word2Vec model and explore embeddings."
      ],
      "metadata": {
        "id": "4txDXCKoLO8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Sample data\n",
        "headlines = [\n",
        "    \"Stock markets rally as tech shares surge\",\n",
        "    \"New species of bird discovered in Amazon rainforest\",\n",
        "    \"Global climate summit ends with landmark agreement\",\n",
        "    \"Breakthrough in quantum computing announced\",\n",
        "    \"Major cybersecurity breach affects millions\"\n",
        "]\n",
        "\n",
        "# 2. Preprocessing\n",
        "def preprocess(docs):\n",
        "    tokens = []\n",
        "    for doc in docs:\n",
        "        ws = word_tokenize(doc)\n",
        "        filt = [w.lower() for w in ws if w.isalpha() and w.lower() not in stop_words]\n",
        "        lem = [lemmatizer.lemmatize(w) for w in filt]\n",
        "        tokens.append(lem)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "processed_headlines = preprocess(headlines)\n",
        "print(\"Processed Headlines:\\n\", processed_headlines)\n",
        "\n",
        "# 3. TF‑IDF Vectorization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfv = TfidfVectorizer()\n",
        "X = tfv.fit_transform(headlines)\n",
        "print(\"TF‑IDF Features shape:\", X.shape)\n",
        "\n",
        "# 4. Word2Vec\n",
        "from gensim.models import Word2Vec\n",
        "lab_w2v = Word2Vec(sentences=processed_headlines, vector_size=20, window=3, min_count=1)\n",
        "print(\"Embedding for 'quantum':\", lab_w2v.wv['quantum'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQjAJpRALasZ",
        "outputId": "a4d90a53-96fc-4aba-b4da-12bdd80dd6a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Headlines:\n",
            " [['stock', 'market', 'rally', 'tech', 'share', 'surge'], ['new', 'specie', 'bird', 'discovered', 'amazon', 'rainforest'], ['global', 'climate', 'summit', 'end', 'landmark', 'agreement'], ['breakthrough', 'quantum', 'computing', 'announced'], ['major', 'cybersecurity', 'breach', 'affect', 'million']]\n",
            "TF‑IDF Features shape: (5, 31)\n",
            "Embedding for 'quantum': [-0.03570098  0.00620068 -0.03589604 -0.01122253  0.01859439  0.0291792\n",
            "  0.0059977   0.01050102 -0.02054807  0.03614411 -0.03154423  0.0232349\n",
            " -0.04109633  0.01017275 -0.02488567 -0.02123807 -0.01554217  0.02826661\n",
            "  0.02899998 -0.02488009]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case Study 1 : Document Clustering"
      ],
      "metadata": {
        "id": "2E7CP3KawLfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is document clustering?\n",
        "- Unsupervised grouping of documents so that texts in the same cluster are more similar to each other than to those in other clusters.\n",
        "\n",
        "## Why use it?\n",
        "\n",
        "* Organize large corpora (e.g., news articles, customer feedback) into coherent themes\n",
        "* Enable fast browsing, topic exploration, or downstream labeling\n",
        "This case study shows:\n",
        "\n",
        "## Preprocessing text with NLTK\n",
        "* Building TF–IDF vectors with Gensim\n",
        "* Clustering with scikit‑learn’s KMeans\n",
        "* Interpreting clusters via top terms and sample docs"
      ],
      "metadata": {
        "id": "XDFuS8-DwPf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import reuters, stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from gensim import corpora, models\n",
        "from gensim.matutils import corpus2csc\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import numpy as np\n",
        "import random\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j8H_YoDwfI1",
        "outputId": "c8af726a-d005-4e4e-8835-390ef1abea4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load & Sample the Reuters Corpus"
      ],
      "metadata": {
        "id": "hqrHrMLCwkcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll take a random sample of 500 Reuters articles for speed\n",
        "all_ids = reuters.fileids()         # The original corpus has 10,369 documents and a vocabulary of 29,930 words.\n",
        "sample_ids = random.sample(all_ids, 500)\n",
        "raw_docs = [reuters.raw(fid) for fid in sample_ids]\n",
        "print(f\"Loaded {len(raw_docs)} documents.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M-x4gFFwnu9",
        "outputId": "8b2d34bd-6672-4ccf-d7f5-8b2c94a9d3ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 500 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(f\"Document {i+1}:\")\n",
        "  print(raw_docs[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89qfOWk1VTg2",
        "outputId": "c73c1344-e57a-4533-fa90-1f9e8a56ceaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "SILICON SYSTEMS INC &lt;SLCN> 2ND QTR MARCH 28\n",
            "  Shr profit five cts vs profit two cts\n",
            "      Net profit 325,000 vs profit 105,000\n",
            "      Revs 19.5 mln vs 16.1 mln\n",
            "      Six Mths\n",
            "      Shr profit nine cts vs loss 35 cts\n",
            "      Net profit 627,000 vs loss 2,280,000\n",
            "      Revs 36.9 mln vs 27.4 mln\n",
            "  \n",
            "\n",
            "\n",
            "Document 2:\n",
            "JAPAN REJECTS U.S. OBJECTIONS TO FAIRCHILD SALE\n",
            "  A Foreign Ministry official dismissed\n",
            "  arguments made by senior U.S. Government officials seeking to\n",
            "  block the sale of a U.S. Microchip maker to a Japanese firm.\n",
            "      \"They appear to be linking completely unrelated issues,\"\n",
            "  Shuichi Takemoto of the Foreign Ministry's North American\n",
            "  Division told Reuters.\n",
            "      U.S. Commerce Secretary Malcolm Baldrige has asked the\n",
            "  White House to consider blocking the sale of &lt;Fairchild\n",
            "  Semiconductor Corp> to Japan's Fujitsu Ltd &lt;ITSU.T>, U.S.\n",
            "  Officials said yesterday.\n",
            "      Baldrige expressed concern that the sale would leave the\n",
            "  U.S. Military dependent on a foreign company for vital high\n",
            "  technology equipment. Pentagon officials said Defence Secretary\n",
            "  Caspar Weinberger also opposes to the sale.\n",
            "      U.S. Officials have also said the sale would give Fujitsu a\n",
            "  powerful role in the U.S. Market for supercomputers while\n",
            "  Japan's supercomputer market remains closed to U.S. Sales.\n",
            "      Takemoto said national security should not be an issue\n",
            "  since the planned purchase of Fairchild from its current owner,\n",
            "  Schlumberger Ltd &lt;SLB>, does not include Fairchild's main\n",
            "  defence-related division.\n",
            "      In addition, Takemoto said tension over the supercomputer\n",
            "  trade should not affect the sale as Fairchild does not make\n",
            "  supercomputers.\n",
            "      Analysts noted that Fairchild does make sophisticated\n",
            "  microchips used in supercomputers. Fujitsu makes similar chips\n",
            "  and supplies them to U.S. Supercomputer makers, they said.\n",
            "      Takemoto also dismissed U.S. Fears that the proposed\n",
            "  takeover would violate U.S. Antitrust law, saying \"the purchase\n",
            "  would not result in Fujitsu monopolising the U.S. Semiconductor\n",
            "  market.\"\n",
            "      Two separate issues appear to have come together to boost\n",
            "  pressure to block the purchase, industry analysts said.\n",
            "      The move is in part an attempt to force Japan to open its\n",
            "  domestic market to more U.S. Supercomputer sales, they said.\n",
            "      U.S. Officials have repeatedly charged that the Japanese\n",
            "  public sector is closed to U.S. Supercomputer sales despite\n",
            "  U.S. Firms' technological lead in the field.\n",
            "      \"The United States believes Japan will only react when\n",
            "  bullied, and this is a bullying ploy,\" Salomon Brothers Asia\n",
            "  analyst Carole Ryavec said.\n",
            "      However, the analysts said more is at stake than\n",
            "  supercomputer sales as the U.S. Fears it is losing its vital\n",
            "  semiconductor industry to Japanese competitors.\n",
            "      \"The real issue is xenophobia in (the U.S.) Silicon Valley,\"\n",
            "  said Tom Murtha of brokerage James Capel and Co.\n",
            "      U.S.-Japanese tension over the semiconductor trade has\n",
            "  failed to subside despite recent efforts by Japan's Ministry of\n",
            "  International Trade and Industry (MITI) to get Japanese firms\n",
            "  to abide by a bilateral pact aimed at halting predatory pricing\n",
            "  and opening Japan's market.\n",
            "      A MITI official said that while Japan is faithfully abiding\n",
            "  by the agreement, problems remain in halting the sale of\n",
            "  microchips in Europe and Southeast Asia at prices below those\n",
            "  set by the pact.\n",
            "      \"It is only a matter of time before we solve this problem,\"\n",
            "  he told Reuters.\n",
            "      Despite the furore, Fujitsu will proceed with talks on the\n",
            "  acquisition in line with the basic agreement reached with\n",
            "  Schlumberger last year, a Fujitsu spokeswoman told Reuters.\n",
            "  \n",
            "\n",
            "\n",
            "Document 3:\n",
            "CHRYSLER &lt;C> DEAL LEAVES UNCERTAINTY FOR AMC WORKERS\n",
            "  Chrysler Corp's 1.5 billion dlr bid to\n",
            "  takeover American Motors Corp &lt;AMO> should help bolster the\n",
            "  small automaker's sales, but it leaves the future of its 19,000\n",
            "  employees in doubt, industry analysts say.\n",
            "      It was \"business as usual\" yesterday at the American Motors\n",
            "  headquarters, one day after the proposed merger was unveiled by\n",
            "  Chrysler and AMC's French parent Renault, according to company\n",
            "  spokesman Edd Snyder.\n",
            "      But AMC's future, to be discussed at a board meeting today,\n",
            "  would be radically different as a Chrysler subsidiary than if\n",
            "  it had continued with the state-run French car group as its\n",
            "  controlling shareholder.\n",
            "      Industry analysts said the future of AMC's car assembly\n",
            "  plant in Kenosha, Wis., and its Toledo, Ohio, Jeep plant would\n",
            "  be in doubt if the overcapacity predicted in the North American\n",
            "  auto industry by the early 1990s comes to pass.\n",
            "      Both plants are far from \"state of the art\" for car\n",
            "  manufacturing sites, and AMC has a history of poor labor\n",
            "  relations at each.\n",
            "      \"Chrysler doesn't need that many new plants,\" said Michael\n",
            "  Luckey, automotive analyst for the Wall Street firm Shearson\n",
            "  Lehman Brothers. \"They probably will close the Toledo plant and\n",
            "  move Jeep production to Canada.\"\n",
            "      Ronald Glantz of Montgomery Securities said that at the\n",
            "  very least, the new owner of the Toledo plant would be able to\n",
            "  wring concessions from the United Automobile Workers union\n",
            "  local representing Jeep workers.\n",
            "      \"The UAW won't be able to hold them up for ransom as they\n",
            "  have AMC because during a down year, Chrysler will have\n",
            "  underutilized facilities to transfer production,\" he said.\n",
            "      Analysts said they foresaw no major complications that\n",
            "  would abort a combination which historians said would be the\n",
            "  auto industry's biggest merger since American Motors was formed\n",
            "  in 1954.\n",
            "      AMC was in need of a financial savior because of its losses\n",
            "  of more than 800 mln dlrs since 1980 and pressures in France\n",
            "  for Renault to cut its backing. The company had said it could\n",
            "  not forecast consistent profitability until 1988 at the\n",
            "  earliest.\n",
            "      In announcing the takeover agreement, Chrysler chairman Lee\n",
            "  Iacocca cited AMC's Jeep division as well as its new 675 mln\n",
            "  dlr assembly plant at Bramalea, Ontario, and its network of\n",
            "  1,200 dealers as the major attractions.\n",
            "      Analysts reasoned that Chrysler might feel moved eventually\n",
            "  to sell off or close some of the older plants to cut overhead\n",
            "  costs in view of the new debts and liabilities it would incur\n",
            "  in the AMC buyout.\n",
            "  \n",
            "\n",
            "\n",
            "Document 4:\n",
            "AUSTRALIAN WHEAT AREA TO FALL, FORECASTER SAYS\n",
            "  Australian wheat plantings are\n",
            "  forecast to fall to 10.40 mln hectares in 1987/88 from 11.72\n",
            "  mln sown in 1986/87, Australian Wheat Forecasters Pty Ltd (AWF)\n",
            "  said in its first preliminary crop forecast.\n",
            "      But there was no reason to expect Australian production in\n",
            "  1987/88 would be less than the 16.5 mln tonnes of last year,\n",
            "  the private forecaster said, as crops in New South Wales and\n",
            "  Queensland suffered from poor yields last season.\n",
            "      Most of the fall in plantings was expected in Western\n",
            "  Australia while state average yields would be assisted by\n",
            "  growers sowing wheat on fallows and rest paddocks, it said.\n",
            "      The main reason for a low Western Australia estimate was a\n",
            "  poor profit outlook under cost, credit and yield pressures. But\n",
            "  in the eastern states the wheat area should hold up provided\n",
            "  that rainfall between now and June is not less than average,\n",
            "  AWF said.\n",
            "      Although some farmers were saying they intended to cut back\n",
            "  wheat area by 20 pct, AWF said this was unlikely since they\n",
            "  needed cash flow and there were problems with alternative\n",
            "  crops.\n",
            "      \"The lack of statutory marketing for oilseeds, pulses and\n",
            "  oats is a cause for concern if those crops are to comprise a\n",
            "  high proportion of growers' income,\" AWF said.\n",
            "      AWF's state area forecasts in mln hectares, with 1986/87\n",
            "  production in mln tonnes, are as follows (crop forecasts were\n",
            "  not given for the new wheat year)\n",
            "                     Area         Crop\n",
            "               1987/88  1986/87  1986/87\n",
            "   Queensland    0.82     0.82     0.95\n",
            "   N.S.W.        3.07     3.17     4.40\n",
            "   Victoria      1.53     1.63     3.25\n",
            "   S.Australia   1.45     1.64     2.30\n",
            "   W.Australia   3.53     4.46     5.60\n",
            "  \n",
            "\n",
            "\n",
            "Document 5:\n",
            "BAKER SAYS G-6 PACT JUST A START\n",
            "  Treasury Secretary James Baker said\n",
            "  the agreement among industrial nations in Paris last month is\n",
            "  only a start in Washington's drive to intensify economic\n",
            "  cooperation among leading countries.\n",
            "      In a speech to the National Newspaper Association, Baker\n",
            "  said \"the six steps beginning with the Plaza Agreement and\n",
            "  culminating in the Paris Accord, are only a start.\"\n",
            "      He added \"we see our role as a steward of a process in which\n",
            "  we sit down with our industrial allies to find ways to promote\n",
            "  more balanced international growth.\"\n",
            "      The Paris agreements called trade surplus countries to\n",
            "  strengthen their growth and on the U.S. to reduce its budget\n",
            "  deficit. Under such circumstances, the countries agreed their\n",
            "  currencies were within ranges broadly consistent with economic\n",
            "  fundamentals.\n",
            "      Baker also said he still sees \"ominous\" signs of pressure for\n",
            "  protectionist trade legislation \"and this pressure for\n",
            "  protectionism is coming from new areas of society.\"\n",
            "      But he also said he believed a coalition was forming that\n",
            "  supported free trade.\n",
            "  \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing with NLTK"
      ],
      "metadata": {
        "id": "hAtuoi7Swr9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return tokens\n",
        "\n",
        "tokenized_docs = [preprocess(doc) for doc in raw_docs]\n"
      ],
      "metadata": {
        "id": "nUHLwCW2wuK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(f\"Document {i+1}:\")\n",
        "  print(tokenized_docs[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mi7gs5lCX2IY",
        "outputId": "6fe72a06-6c8d-48d2-e89f-1efb541904a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "['silicon', 'system', 'inc', 'lt', 'slcn', 'qtr', 'march', 'shr', 'profit', 'five', 'ct', 'v', 'profit', 'two', 'ct', 'net', 'profit', 'v', 'profit', 'rev', 'mln', 'v', 'mln', 'six', 'mths', 'shr', 'profit', 'nine', 'ct', 'v', 'loss', 'ct', 'net', 'profit', 'v', 'loss', 'rev', 'mln', 'v', 'mln']\n",
            "Document 2:\n",
            "['japan', 'reject', 'objection', 'fairchild', 'sale', 'foreign', 'ministry', 'official', 'dismissed', 'argument', 'made', 'senior', 'government', 'official', 'seeking', 'block', 'sale', 'microchip', 'maker', 'japanese', 'firm', 'appear', 'linking', 'completely', 'unrelated', 'issue', 'shuichi', 'takemoto', 'foreign', 'ministry', 'north', 'american', 'division', 'told', 'reuters', 'commerce', 'secretary', 'malcolm', 'baldrige', 'asked', 'white', 'house', 'consider', 'blocking', 'sale', 'lt', 'fairchild', 'semiconductor', 'corp', 'japan', 'fujitsu', 'ltd', 'lt', 'official', 'said', 'yesterday', 'baldrige', 'expressed', 'concern', 'sale', 'would', 'leave', 'military', 'dependent', 'foreign', 'company', 'vital', 'high', 'technology', 'equipment', 'pentagon', 'official', 'said', 'defence', 'secretary', 'caspar', 'weinberger', 'also', 'opposes', 'sale', 'official', 'also', 'said', 'sale', 'would', 'give', 'fujitsu', 'powerful', 'role', 'market', 'supercomputer', 'japan', 'supercomputer', 'market', 'remains', 'closed', 'sale', 'takemoto', 'said', 'national', 'security', 'issue', 'since', 'planned', 'purchase', 'fairchild', 'current', 'owner', 'schlumberger', 'ltd', 'lt', 'slb', 'include', 'fairchild', 'main', 'division', 'addition', 'takemoto', 'said', 'tension', 'supercomputer', 'trade', 'affect', 'sale', 'fairchild', 'make', 'supercomputer', 'analyst', 'noted', 'fairchild', 'make', 'sophisticated', 'microchip', 'used', 'supercomputer', 'fujitsu', 'make', 'similar', 'chip', 'supply', 'supercomputer', 'maker', 'said', 'takemoto', 'also', 'dismissed', 'fear', 'proposed', 'takeover', 'would', 'violate', 'antitrust', 'law', 'saying', 'purchase', 'would', 'result', 'fujitsu', 'monopolising', 'semiconductor', 'market', 'two', 'separate', 'issue', 'appear', 'come', 'together', 'boost', 'pressure', 'block', 'purchase', 'industry', 'analyst', 'said', 'move', 'part', 'attempt', 'force', 'japan', 'open', 'domestic', 'market', 'supercomputer', 'sale', 'said', 'official', 'repeatedly', 'charged', 'japanese', 'public', 'sector', 'closed', 'supercomputer', 'sale', 'despite', 'firm', 'technological', 'lead', 'field', 'united', 'state', 'belief', 'japan', 'react', 'bullied', 'bullying', 'ploy', 'salomon', 'brother', 'asia', 'analyst', 'carole', 'ryavec', 'said', 'however', 'analyst', 'said', 'stake', 'supercomputer', 'sale', 'fear', 'losing', 'vital', 'semiconductor', 'industry', 'japanese', 'competitor', 'real', 'issue', 'xenophobia', 'silicon', 'valley', 'said', 'tom', 'murtha', 'brokerage', 'james', 'capel', 'tension', 'semiconductor', 'trade', 'failed', 'subside', 'despite', 'recent', 'effort', 'japan', 'ministry', 'international', 'trade', 'industry', 'miti', 'get', 'japanese', 'firm', 'abide', 'bilateral', 'pact', 'aimed', 'halting', 'predatory', 'pricing', 'opening', 'japan', 'market', 'miti', 'official', 'said', 'japan', 'faithfully', 'abiding', 'agreement', 'problem', 'remain', 'halting', 'sale', 'microchip', 'europe', 'southeast', 'asia', 'price', 'set', 'pact', 'matter', 'time', 'solve', 'problem', 'told', 'reuters', 'despite', 'furore', 'fujitsu', 'proceed', 'talk', 'acquisition', 'line', 'basic', 'agreement', 'reached', 'schlumberger', 'last', 'year', 'fujitsu', 'spokeswoman', 'told', 'reuters']\n",
            "Document 3:\n",
            "['chrysler', 'lt', 'c', 'deal', 'leaf', 'uncertainty', 'amc', 'worker', 'chrysler', 'corp', 'billion', 'dlr', 'bid', 'takeover', 'american', 'motor', 'corp', 'lt', 'amo', 'help', 'bolster', 'small', 'automaker', 'sale', 'leaf', 'future', 'employee', 'doubt', 'industry', 'analyst', 'say', 'business', 'usual', 'yesterday', 'american', 'motor', 'headquarters', 'one', 'day', 'proposed', 'merger', 'unveiled', 'chrysler', 'amc', 'french', 'parent', 'renault', 'according', 'company', 'spokesman', 'edd', 'snyder', 'amc', 'future', 'discussed', 'board', 'meeting', 'today', 'would', 'radically', 'different', 'chrysler', 'subsidiary', 'continued', 'french', 'car', 'group', 'controlling', 'shareholder', 'industry', 'analyst', 'said', 'future', 'amc', 'car', 'assembly', 'plant', 'kenosha', 'toledo', 'ohio', 'jeep', 'plant', 'would', 'doubt', 'overcapacity', 'predicted', 'north', 'american', 'auto', 'industry', 'early', 'come', 'pas', 'plant', 'far', 'state', 'art', 'car', 'manufacturing', 'site', 'amc', 'history', 'poor', 'labor', 'relation', 'chrysler', 'need', 'many', 'new', 'plant', 'said', 'michael', 'luckey', 'automotive', 'analyst', 'wall', 'street', 'firm', 'shearson', 'lehman', 'brother', 'probably', 'close', 'toledo', 'plant', 'move', 'jeep', 'production', 'canada', 'ronald', 'glantz', 'montgomery', 'security', 'said', 'least', 'new', 'owner', 'toledo', 'plant', 'would', 'able', 'wring', 'concession', 'united', 'automobile', 'worker', 'union', 'local', 'representing', 'jeep', 'worker', 'uaw', 'wo', 'able', 'hold', 'ransom', 'amc', 'year', 'chrysler', 'underutilized', 'facility', 'transfer', 'production', 'said', 'analyst', 'said', 'foresaw', 'major', 'complication', 'would', 'abort', 'combination', 'historian', 'said', 'would', 'auto', 'industry', 'biggest', 'merger', 'since', 'american', 'motor', 'formed', 'amc', 'need', 'financial', 'savior', 'loss', 'mln', 'dlrs', 'since', 'pressure', 'france', 'renault', 'cut', 'backing', 'company', 'said', 'could', 'forecast', 'consistent', 'profitability', 'earliest', 'announcing', 'takeover', 'agreement', 'chrysler', 'chairman', 'lee', 'iacocca', 'cited', 'amc', 'jeep', 'division', 'well', 'new', 'mln', 'dlr', 'assembly', 'plant', 'bramalea', 'ontario', 'network', 'dealer', 'major', 'attraction', 'analyst', 'reasoned', 'chrysler', 'might', 'feel', 'moved', 'eventually', 'sell', 'close', 'older', 'plant', 'cut', 'overhead', 'cost', 'view', 'new', 'debt', 'liability', 'would', 'incur', 'amc', 'buyout']\n",
            "Document 4:\n",
            "['australian', 'wheat', 'area', 'fall', 'forecaster', 'say', 'australian', 'wheat', 'planting', 'forecast', 'fall', 'mln', 'hectare', 'mln', 'sown', 'australian', 'wheat', 'forecaster', 'pty', 'ltd', 'awf', 'said', 'first', 'preliminary', 'crop', 'forecast', 'reason', 'expect', 'australian', 'production', 'would', 'less', 'mln', 'tonne', 'last', 'year', 'private', 'forecaster', 'said', 'crop', 'new', 'south', 'wale', 'queensland', 'suffered', 'poor', 'yield', 'last', 'season', 'fall', 'planting', 'expected', 'western', 'australia', 'state', 'average', 'yield', 'would', 'assisted', 'grower', 'sowing', 'wheat', 'fallow', 'rest', 'paddock', 'said', 'main', 'reason', 'low', 'western', 'australia', 'estimate', 'poor', 'profit', 'outlook', 'cost', 'credit', 'yield', 'pressure', 'eastern', 'state', 'wheat', 'area', 'hold', 'provided', 'rainfall', 'june', 'less', 'average', 'awf', 'said', 'although', 'farmer', 'saying', 'intended', 'cut', 'back', 'wheat', 'area', 'pct', 'awf', 'said', 'unlikely', 'since', 'needed', 'cash', 'flow', 'problem', 'alternative', 'crop', 'lack', 'statutory', 'marketing', 'oilseed', 'pulse', 'oat', 'cause', 'concern', 'crop', 'comprise', 'high', 'proportion', 'grower', 'income', 'awf', 'said', 'awf', 'state', 'area', 'forecast', 'mln', 'hectare', 'production', 'mln', 'tonne', 'follows', 'crop', 'forecast', 'given', 'new', 'wheat', 'year', 'area', 'crop', 'queensland', 'victoria']\n",
            "Document 5:\n",
            "['baker', 'say', 'pact', 'start', 'treasury', 'secretary', 'james', 'baker', 'said', 'agreement', 'among', 'industrial', 'nation', 'paris', 'last', 'month', 'start', 'washington', 'drive', 'intensify', 'economic', 'cooperation', 'among', 'leading', 'country', 'speech', 'national', 'newspaper', 'association', 'baker', 'said', 'six', 'step', 'beginning', 'plaza', 'agreement', 'culminating', 'paris', 'accord', 'start', 'added', 'see', 'role', 'steward', 'process', 'sit', 'industrial', 'ally', 'find', 'way', 'promote', 'balanced', 'international', 'growth', 'paris', 'agreement', 'called', 'trade', 'surplus', 'country', 'strengthen', 'growth', 'reduce', 'budget', 'deficit', 'circumstance', 'country', 'agreed', 'currency', 'within', 'range', 'broadly', 'consistent', 'economic', 'fundamental', 'baker', 'also', 'said', 'still', 'see', 'ominous', 'sign', 'pressure', 'protectionist', 'trade', 'legislation', 'pressure', 'protectionism', 'coming', 'new', 'area', 'society', 'also', 'said', 'believed', 'coalition', 'forming', 'supported', 'free', 'trade']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Gensim TF–IDF Representation"
      ],
      "metadata": {
        "id": "eMmEW3aYww4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create Dictionary & filter extremes\n",
        "dictionary = corpora.Dictionary(tokenized_docs)\n",
        "# dictionary.filter_extremes(no_below=5, no_above=0.5). # removes words that appear in fewer than 5 documents and removes words that appear in more than 50% of documents\n",
        "print(\"Vocabulary Size\",len(dictionary))\n",
        "# 2. Convert to Bag‑of‑Words\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "# 3. Fit TF–IDF model\n",
        "tfidf_model = models.TfidfModel(bow_corpus). # TF-IDF helps highlight words that are unique to a document and downplays words that are common across many documents\n",
        "tfidf_corpus = tfidf_model[bow_corpus]\n",
        "\n",
        "# 4. Create document‑term matrix (sparse CSC) and transpose to (n_docs × n_terms)\n",
        "sparse_matrix = corpus2csc(tfidf_corpus, num_terms=len(dictionary)).transpose()     #  converts the list of TF-IDF vectors into a Compressed Sparse Column (CSC) matrix\n",
        "X = sparse_matrix.toarray()  # dense for KMeans\n",
        "print(\"Feature matrix shape:\", X.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRupl16cwzEz",
        "outputId": "cfb70975-ae2f-4dc4-81a1-22e963cddc02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size 5484\n",
            "Feature matrix shape: (500, 5484)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K‑Means Clustering"
      ],
      "metadata": {
        "id": "e0cMyq_kw1Ql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters = 8\n",
        "km = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "labels = km.fit_predict(X)\n"
      ],
      "metadata": {
        "id": "yD9j50RMw3TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Inspecting Clusters\n",
        "## Top Terms per Cluster"
      ],
      "metadata": {
        "id": "wd2qHC9Ww5qD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = dictionary\n",
        "\n",
        "for i in range(n_clusters):\n",
        "    top_terms = [terms[id] for id in order_centroids[i, :8]]\n",
        "    print(f\"Cluster {i+1} top terms: {', '.join(top_terms)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDwm7Wz7w_G3",
        "outputId": "ed5d4021-36b0-4205-b260-f1c4bbf76c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 1 top terms: share, company, pct, dlrs, stock, inc, offer, quarter\n",
            "Cluster 2 top terms: v, net, ct, mln, shr, qtr, rev, dlrs\n",
            "Cluster 3 top terms: qtly, div, ct, april, prior, record, pay, payout\n",
            "Cluster 4 top terms: tonne, export, trade, wheat, corn, official, shipment, sugar\n",
            "Cluster 5 top terms: gold, mine, ounce, acre, copper, property, mining, mineral\n",
            "Cluster 6 top terms: bank, billion, rate, pct, fed, reserve, mark, dollar\n",
            "Cluster 7 top terms: loss, v, profit, ct, net, rev, shr, mln\n",
            "Cluster 8 top terms: oil, gas, crude, barrel, bpd, well, texas, exploration\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Documents per Cluster"
      ],
      "metadata": {
        "id": "tGdeSwnlxBhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(n_clusters):\n",
        "    print(f\"\\n--- Cluster {i+1} Examples ---\")\n",
        "    # pick 2 random docs from this cluster\n",
        "    doc_indices = np.where(labels == i)[0]\n",
        "    for idx in random.sample(list(doc_indices), 2):\n",
        "        snippet = raw_docs[idx][:200].replace('\\n',' ')\n",
        "        print(f\"• …{snippet}…\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a2jlMCSxJtN",
        "outputId": "cc7e7bba-89e1-4f24-e2f4-e0cf1e741f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Cluster 1 Examples ---\n",
            "• …ROTTERDAM GRAIN HANDLER SAYS PORT BALANCE ROSE   Graan Elevator Mij, GEM, said its   balance in port of grains, oilseeds and derivatives rose to   146,000 tonnes on April 11 from 111,000 a week earlie…\n",
            "• …EC EXPORT LICENCES FOR 59,000 TONNES WHITE  SUGAR AT REBATE 45.678 ECUS - FRENCH TRADERS    EC EXPORT LICENCES FOR 59,000 TONNES WHITE  SUGAR AT REBATE 45.678 ECUS - FRENCH TRADERS     …\n",
            "\n",
            "--- Cluster 2 Examples ---\n",
            "• …INTELLICORP &lt;INAI.O> 1ST QTR SEPT 30 LOSS   Shr loss nine cts vs loss 12 cts       Net loss 649,000 vs loss 850,000       Revs 5,059,000 vs 4,084,000       Avg shrs 7,041,000 vs 6,900,000       NOT…\n",
            "• …RPC ENERGY SERVICES INC &lt;RES> 1ST QTR SEPT 30   Shr profit one cent vs loss 29 cts       Net profit 116,000 vs loss 4,195,000       Revs 20.2 mln vs 6,393,000     …\n",
            "\n",
            "--- Cluster 3 Examples ---\n",
            "• …J.W. MAYS INC &lt;MAYS> 2ND QTR JAN 31 NET   Shr 2.27 dlrs vs 74 cts       Net 4,945,989 vs 1,612,624       Revs 28.2 mln vs 27.9 mln       Six mths       Shr 1.57 dlrs vs three cts       Net 3,417,65…\n",
            "• …PHH GROUP INC &lt;PHH> 4TH QTR APRIL 30 NET   Shr 71 cts vs 47 cts       Net 12.1 mln vs 7.8 mln       Revs 369.8 mln vs 307.9 mln       12 mths       Shr 2.35 dlrs vs 2.33 dlrs       Net 39.5 mln vs …\n",
            "\n",
            "--- Cluster 4 Examples ---\n",
            "• …SOVIET 1988 OIL OUTPUT TARGET AT 625 MLN TONNES   The Soviet oil production target for 1988   has been set at 625 mln tonnes, a rise of eight mln tonnes over   this year's planned output.       Oil Mi…\n",
            "• …COLOMBIA TRADERS SAY NEW COFFEE STRATEGY VITAL   Coffee producing countries must quickly   map out a fresh common strategy following the failure of the   International Coffee Organization, ICO, to rea…\n",
            "\n",
            "--- Cluster 5 Examples ---\n",
            "• …BAKER/STOLTENBERG MEETING SOOTHES MARKETS   News of a meeting between U.S. Treasury   Secretary James Baker and West German Finance Minister Gerhard   Stoltenberg on Monday soothed currency markets, a…\n",
            "• …BELGIAN UNEMPLOYMENT FALLS IN MARCH   Belgian unemployment, based on the   number of jobless drawing unemployment benefit, fell to 11.8   pct of the working population at the end of March from 12.1 pc…\n",
            "\n",
            "--- Cluster 6 Examples ---\n",
            "• …MONTGOMERY STREET INCOME &lt;MTS> MONTHLY DIVIDEND   Mthly div 15 cts vs 15 cts       Pay April 15       Record April 1     …\n",
            "• …HELIG-MEYERS CO &lt;HMY> INCREASES DIVIDEND   Qtly div eight cts vs seven cts prior       Payable May 15       Record APril 29     …\n",
            "\n",
            "--- Cluster 7 Examples ---\n",
            "• …SOUTHWESTERN BELL&lt;SBC> VOTES SPLIT, UPS PAYOUT   Southwestern Bell Corp said its board   voted a three-for-one stock split and increased the dividend   8.8 pct to 1.60 dlrs a share.       On a post…\n",
            "• …ROCHESTER TELEPHONE &lt;RTC> TO BUY CANTON PHONE   Rochester Telephone Corp said   it agreed to buy &lt;Canton Telephone Co> for undisclosed terms.       Canton serves customers in northeastern Penn. …\n",
            "\n",
            "--- Cluster 8 Examples ---\n",
            "• …DUTCH ADJUSTED UNEMPLOYMENT UNCHANGED IN FEBRUARY   Dutch seasonally-adjusted   unemployment totalled 690,600 people in February, unchanged   from January but down from 732,700 in February 1986, a Soc…\n",
            "• …FINANCE MINISTERS AGREE ON NEED FOR STABILITY   Finance ministers from seven major   industrialized nations agreed on the need to stabilize   currencies at current levels but said more action was need…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case Study 2 : Sentiment Analysis for Movie Reviews\n",
        "\n",
        "* Goal: Uncover the main themes (topics) in a large set of movie reviews and explore semantic relationships between words.\n",
        "* Why it matters: Topic modeling helps condense thousands of customer opinions into a handful of interpretable themes (e.g., “acting,” “plot,” “special effects”). Word embeddings (Word2Vec) let us zoom into nuanced semantic clusters (e.g., “thrilling” ≈ “suspenseful”).\n",
        "\n",
        "* Takeaways:\n",
        "\n",
        "How to preprocess text at scale with NLTK\n",
        "How to build TF–IDF and LDA topic models with Gensim\n",
        "How to train and query a Word2Vec model"
      ],
      "metadata": {
        "id": "7dPA0p8UvLOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Dataset"
      ],
      "metadata": {
        "id": "vosI5Omfvfux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "# Load documents as lists of words\n",
        "docs = [movie_reviews.words(fileid) for fileid in movie_reviews.fileids()]\n",
        "print(f\"Loaded {len(docs)} reviews, average length ~{sum(len(d) for d in docs)/len(docs):.0f} tokens.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI6hb-sjvjac",
        "outputId": "14d75d5f-2a3c-4331-9745-b31d4f1b4666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2000 reviews, average length ~792 tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Preprocessing with NLTK"
      ],
      "metadata": {
        "id": "LbN-BYjMvmLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(doc):\n",
        "    # join tokens to string, then re-tokenize to handle punctuation properly\n",
        "    text = \" \".join(doc).lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [\n",
        "        lemmatizer.lemmatize(tok)\n",
        "        for tok in tokens\n",
        "        if tok.isalpha() and tok not in stop_words\n",
        "    ]\n",
        "    return tokens\n",
        "\n",
        "processed_docs = [preprocess(doc) for doc in docs]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg3S1UDCvodK",
        "outputId": "746b045b-83b0-4a7e-ffae-5ffd4bdbf73e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. TF–IDF Analysis"
      ],
      "metadata": {
        "id": "tkiYWc-7vrh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora, models\n",
        "\n",
        "# 1. Build dictionary and filter extremes\n",
        "dictionary = corpora.Dictionary(processed_docs)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
        "\n",
        "# 2. Convert to BOW corpus\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "# 3. Fit TF–IDF model\n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "tfidf_corpus = tfidf[bow_corpus]\n",
        "\n",
        "# Display top‑5 TF–IDF words in the first review\n",
        "doc0 = tfidf_corpus[0]\n",
        "top5 = sorted(doc0, key=lambda x: -x[1])[:5]\n",
        "print(\"Top 5 TF–IDF in review #1:\")\n",
        "for term_id, score in top5:\n",
        "    print(f\"  {dictionary[term_id]} — {score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFELrlhWvu4J",
        "outputId": "5d9496d5-c8ae-463a-8ff0-a6d947140065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 TF–IDF in review #1:\n",
            "  strangeness — 0.229\n",
            "  teen — 0.211\n",
            "  fuck — 0.190\n",
            "  highway — 0.178\n",
            "  crow — 0.169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Topic Modeling with LDA"
      ],
      "metadata": {
        "id": "SeG8yL_wvz3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a small LDA model\n",
        "lda = models.LdaModel(\n",
        "    corpus=bow_corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=6,\n",
        "    passes=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Show the 6 topics\n",
        "for tid in range(6):\n",
        "    terms = lda.show_topic(tid, topn=8)\n",
        "    print(f\"Topic {tid+1}: \" + \", \".join([term for term, _ in terms]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S00tjp5Qv17-",
        "outputId": "e568d88f-988c-4ad2-87f7-3117fe8d62f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: thing, really, life, bad, plot, know, take, could\n",
            "Topic 2: bad, life, plot, really, director, actor, look, big\n",
            "Topic 3: life, performance, man, u, work, many, action, come\n",
            "Topic 4: star, action, effect, plot, bad, life, u, take\n",
            "Topic 5: funny, comedy, thing, know, little, people, come, day\n",
            "Topic 6: life, people, little, really, come, alien, never, know\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Semantic Similarity with Word2Vec"
      ],
      "metadata": {
        "id": "GPcF4PpAv40i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train Word2Vec on the processed docs\n",
        "w2v = Word2Vec(\n",
        "    sentences=processed_docs,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=10,\n",
        "    epochs=20,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Find similar words\n",
        "for target in [\"plot\", \"actor\", \"love\", \"horror\"]:\n",
        "    if target in w2v.wv:\n",
        "        sims = w2v.wv.most_similar(target, topn=5)\n",
        "        print(f\"\\nTop words similar to '{target}':\")\n",
        "        for word, score in sims:\n",
        "            print(f\"  {word} — {score:.3f}\")\n",
        "    else:\n",
        "        print(f\"'{target}' not in vocabulary.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhDCMI-5v62N",
        "outputId": "43a4b7d0-86fa-4368-f637-4c6c9c67e44a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top words similar to 'plot':\n",
            "  storyline — 0.542\n",
            "  gaping — 0.517\n",
            "  premise — 0.487\n",
            "  confusing — 0.484\n",
            "  coincidence — 0.443\n",
            "\n",
            "Top words similar to 'actor':\n",
            "  performer — 0.642\n",
            "  actress — 0.639\n",
            "  talented — 0.522\n",
            "  cast — 0.507\n",
            "  acting — 0.460\n",
            "\n",
            "Top words similar to 'love':\n",
            "  asleep — 0.638\n",
            "  flat — 0.546\n",
            "  apart — 0.528\n",
            "  prey — 0.473\n",
            "  category — 0.455\n",
            "\n",
            "Top words similar to 'horror':\n",
            "  slasher — 0.690\n",
            "  genre — 0.534\n",
            "  scary — 0.532\n",
            "  exorcist — 0.504\n",
            "  erotic — 0.500\n"
          ]
        }
      ]
    }
  ]
}